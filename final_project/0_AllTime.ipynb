{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 EDA\n",
    "\n",
    "| 구분        | 개수  | fake여부   | 기반   | 사이즈     | train(%) | val(%) | test(%) |\n",
    "| ----------- | ----- | ---------- | ------ | ---------- | -------- | ------ | ------- |\n",
    "| FFHQ        | 35000 | 0          |        | 1024, 1024 | 70       | 10     | 20      |\n",
    "| LFW         | 13233 | 0          |        | 250, 250   | 70       | 10     | 20      |\n",
    "| CelebA      | 17767 | 0          |        | 178, 218   | 70       | 10     | 20      |\n",
    "| StarGAN     | 6000  | 1          | CelebA | 1024, 1024 | 70       | 10     | 20      |\n",
    "| StyleGAN1   | 30000 | 1          | FFHQ   | 1024, 1024 | 70       | 10     | 20      |\n",
    "| StyleGAN2   | 30000 | 1          | FFHQ   | 1024, 1024 | 70       | 10     | 20      |\n",
    "| StyleGAN-XL | 13200 | 1 (unseen) | FFHQ   | 256, 256   | 0        | 0      | 100     |\n",
    "\n",
    "\n",
    "\n",
    "각 데이터는  아래와 같은 형태로 저장되어 있다.\n",
    "\n",
    "```\n",
    "dataset\n",
    "├── CelebA\n",
    "├── FFHQ\n",
    "├── LFW\n",
    "├── StarGAN\n",
    "├── StyleGAN1\n",
    "├── StyleGAN2\n",
    "└── StyleGANXL\n",
    "```\n",
    "real, fake 얼굴 사진 각각 66,000 장을 사용한다. StyleGANXL 데이터는 학습에 사용하지 않고, 테스트 시에만 사용한다.\n",
    "\n",
    "학습 과정에서 모든 데이터를 램에 올리기에는 데이터의 양이 너무 많기 때문에 train, val, test 데이터 들의 경로가 포함된  csv 파일을 만들어준다.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCSV(path, label, tag, dst=None):\n",
    "    \"\"\"\n",
    "    path: 사진이 들어있는 폴더의 경로\n",
    "    label: 0이면 진짜 사진, 1이면 가짜 사진\n",
    "    tag: 이후에 분석을 쉽게 하기 위해 폴더 이름을 tag로 넣어줌\n",
    "\n",
    "    return: path 내부에 있는 모든 사진의 경로와 label, tag를 행으로 갖는 csv 파일\n",
    "    \"\"\"\n",
    "    path = pathlib.PosixPath(path) # path 설정\n",
    "    generator = path.glob(\"./*\") # 경로 이하의 파일을 뱉는 generator 생성\n",
    "    nullList = list() # 빈 파일 경로가 들어있는 리스트\n",
    "    fileList = list() # 정상적인 파일의 경로가 들어있는 리스트\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        try:\n",
    "            filepath = generator.__next__() # 파일의 경로를 하나씩 받아온다\n",
    "            if os.stat(filepath).st_size == 0: # 파일 사이즈가 0인 null file을 검사한다\n",
    "                nullList.append(filepath)\n",
    "            else:\n",
    "                fileList.append(filepath)\n",
    "        except: # 더 이상 남은 파일이 없으면 while 문을 빠져나간다\n",
    "            break\n",
    "        if i % 1000 == 0: # 1000 간격으로 진행도를 보여준다\n",
    "            clear_output()\n",
    "            print(i)\n",
    "    csv = pd.DataFrame({\n",
    "        'filepath': fileList,\n",
    "        'label': label,\n",
    "        'tag': tag\n",
    "    })\n",
    "\n",
    "    if dst: # 저장할 경로가 존재하면 저장하고 리턴값은 None\n",
    "        csv.to_csv(dst)\n",
    "        return\n",
    "    \n",
    "    return csv, nullList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirList = list(pathlib.PosixPath(\"./dataset\").glob(\"./*\"))\n",
    "\n",
    "# 아래 값을 적절하게 조절해서 csv 폴더 생성, dirList에는 dataset 폴더들의 이름이 들어 있음\n",
    "makeCSV(dirList[0], label=0, tag=\"FFHQ\", dst=f\"./csv/{dirList[0].name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvList = list(pathlib.PosixPath(\"./csv/\").glob(\"./*.csv\"))\n",
    "# 여기에 styleGAN-XL에 해당하는 index만 pop으로 솎아준다\n",
    "csvList.pop(4)\n",
    "\n",
    "# XL을 제외한 모든 데이터들을 넣어준다\n",
    "df = pd.DataFrame()\n",
    "for csv in csvList:\n",
    "    temp = pd.read_csv(csv, index_col=0)\n",
    "    df = pd.concat([df, temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금은 안 섞인 상태니까 한번 섞어준다\n",
    "train, test = train_test_split(df ,test_size=0.2, shuffle=True)\n",
    "\n",
    "train, val = train_test_split(train, test_size=0.125)\n",
    "\n",
    "train.to_csv(\"./csv/train.csv\")\n",
    "val.to_csv(\"./csv/val.csv\")\n",
    "test.to_csv(\"./csv/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XL 전용 테스트 \n",
    "xl = pd.read_csv(\"./csv/StyleGANXL\", index_col=0)\n",
    "xl = xl.sample(13200)\n",
    "test = test[test.label == 0]\n",
    "\n",
    "test_xl = pd.concat([xl, test], axis=0)\n",
    "# 한번 섞어서 저장한다\n",
    "test_xl = test_xl.sample(len(test_xl))\n",
    "test_xl.to_csv(\"./csv/test_XL.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 1. 데이터 전처리 - Benford First Digits Probability Distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 이미지를 DCT 를 이용해 전처리하고 한 이미지에서 각 digit의 첫 번쨰 숫자 빈도를 센다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "def compute_first_digits(img, normalize=True):\n",
    "    # 이미지를 gray scale로 읽어옴\n",
    "    if isinstance(img, str):\n",
    "        img = cv2.imread(img, 0)\n",
    "    # 컬러 이미지가 들어오면 gray scale 로 바꿔줌\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "\n",
    "    # dct 는 shape이 짝수여야 한다고 함, 홀수면 처리를 해줌\n",
    "    if img.shape[0] % 2:\n",
    "        img = img[:img.shape[0] - 1,:]\n",
    "    \n",
    "    if img.shape[1] % 2:\n",
    "        img = img[:,:img.shape[1] - 1]\n",
    "\n",
    "    # 0 ~ 255 로 normalize \n",
    "    if normalize:\n",
    "        img = cv2.normalize(img, np.zeros(img.shape), 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # 0 ~ 1 로 다시 normalize 후 descrete cosine transform, 어짜피 First Digit 만 구할꺼니 abs 까지\n",
    "    img = cv2.dct(np.float32(img) / 255.0)\n",
    "    img = np.abs(img)\n",
    "\n",
    "    # 0 인 경우 로그에 들어갈 때 문제가 생김\n",
    "    img = img[img != 0]\n",
    "    min_val = img.min()\n",
    "\n",
    "    if min_val < 1:\n",
    "        img = np.power(10, -np.floor(np.log10(min_val))) * img\n",
    "\n",
    "    # 위 코드를 지나면 모두 1.0 보다 커야함\n",
    "    if not (img >= 1.0).all():\n",
    "        raise ValueError(\"Error\")\n",
    "    \n",
    "    # [1, 10) 은 0, [10, 100) 은 1, 이런느낌\n",
    "    digits = np.log10(img).astype(int).astype('float32')\n",
    "    first_digits = img / np.power(10, digits)\n",
    "    first_digits = first_digits.astype(int)\n",
    "\n",
    "    # 혹시라도 0이 들어있는 부분이 있다면 예외처리해준다\n",
    "    first_digits = first_digits[first_digits != 0]\n",
    "\n",
    "    \n",
    "    return first_digits\n",
    "\n",
    "def compute_first_digits_counts(img, normalise=True):\n",
    "    first_digits = compute_first_digits(img, normalise)\n",
    "    unq, counts = np.unique(first_digits, return_counts=True)\n",
    "    return unq, counts\n",
    "\n",
    "def compute_first_digits_prob(img, normalise=True):\n",
    "    unq, counts = compute_first_digits_counts(img, normalise)\n",
    "    counts = counts / sum(counts)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train val test XL 각각의 파일경로들을 불러온다\n",
    "train_filepath = pd.read_csv(\"./csv/train.csv\", index_col=0).filepath.values\n",
    "val_filepath = pd.read_csv(\"./csv/val.csv\", index_col=0).filepath.values\n",
    "test_filepath = pd.read_csv(\"./csv/test.csv\", index_col=0).filepath.values\n",
    "test_XL_filepath = pd.read_csv(\"./csv/test_XL.csv\", index_col=0).filepath.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러온 경로들을 사용해 위에서 미리 정의한 함수들을 사용해서 확률분포로 전처리해줌\n",
    "train = np.array([compute_first_digits_prob(i) for i in train_filepath])\n",
    "val = np.array([compute_first_digits_prob(i) for i in val_filepath])\n",
    "test = np.array([compute_first_digits_prob(i) for i in test_filepath])\n",
    "test_XL = np.array([compute_first_digits_prob(i) for i in test_XL_filepath])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy 폴더에 저장\n",
    "np.save(\"./npy/train\", train)\n",
    "np.save(\"./npy/val\", val)\n",
    "np.save(\"./npy/test\", test)\n",
    "np.save(\"./npy/test_XL\", test_XL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리가 완료된 확률분포를 불러옴\n",
    "x_train = np.load(\"./npy/train\")\n",
    "x_val = np.load(\"./npy/val\")\n",
    "x_test = np.load(\"./npy/test\")\n",
    "x_test_XL = np.load(\"./npy/test_XL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label도 따로 불러옴\n",
    "y_train = pd.read_csv(\"./csv/train.csv\", index_col=0).label.values\n",
    "y_val = pd.read_csv(\"./csv/val.csv\", index_col=0).label.values\n",
    "y_test = pd.read_csv(\"./csv/test.csv\", index_col=0).label.values\n",
    "y_test_XL = pd.read_csv(\"./csv/test_XL.csv\", index_col=0).label.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, val 을 합쳐준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate([x_train, x_val], axis=0)\n",
    "y_train = np.concatenate([y_train, y_val], axis=0)\n",
    "del x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix as cm, classification_report as cr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리한 결과를 이용해 머신러닝 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(verbose=1, n_jobs=-1)\n",
    "rf.fit(x_train, y_train)\n",
    "y_rf_pred = rf.predict(x_test)\n",
    "y_rf_proba = rf.predict_proba(x_test)\n",
    "\n",
    "print(cm(y_true=y_test, y_pred=y_rf_pred))\n",
    "print(cr(y_true=y_test, y_pred=y_rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc =  SVC(random_state=0,verbose=1,probability=True)\n",
    "svc.fit(x_train, y_train)\n",
    "y_svc_pred = svc.predict(x_test)\n",
    "y_svc_proba = svc.predict_proba(x_test)\n",
    "\n",
    "print(cm(y_true=y_test, y_pred=y_svc_pred))\n",
    "print(cr(y_true=y_test, y_pred=y_svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50, 20), \n",
    "    verbose=1, \n",
    "    early_stopping=True, \n",
    ")\n",
    "\n",
    "mlp.fit(x_train, x_test)\n",
    "y_mlp_pred = mlp.predict(x_test)\n",
    "y_mlp_proba = mlp.predict_proba(x_test)\n",
    "\n",
    "print(cm(y_true=y_test, y_pred=y_mlp_pred))\n",
    "print(cr(y_true=y_test, y_pred=y_mlp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XL 데이터의 예측 값\n",
    "y_rf_proba_XL = rf.predict_proba(y_test_XL)\n",
    "y_svc_proba_XL = svc.predict_proba(y_test_XL)\n",
    "y_mlp_proba_XL = mlp.predict_proba(y_test_XL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./npy/ML_prob.npz\", rf_pred=y_rf_proba, svc_pred=y_svc_proba, mlp_pred=y_mlp_proba,\n",
    "rf_pred_XL=y_rf_proba_XL, svc_pred_XL=y_svc_proba_XL, mlp_pred_XL=y_mlp_proba_XL,\n",
    "label = y_test, label_XL =y_test_XL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a797451009ee20ba7dc666c3cef2712a6d0583a389ea6c0424476199fc279815"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
